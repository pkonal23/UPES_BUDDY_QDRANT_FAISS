{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nDQjLZNfk1FS",
        "3msGJ11omYli",
        "oG_U1kMUmgpq",
        "llFTCdwcm1dx",
        "EteoF5aFnF7K",
        "pEh0m5s2nVp5",
        "lkQiVY3LnmRB",
        "ueS3RtcxoN7C",
        "G0ucX816qTLC",
        "dHGOLh8XqZfy",
        "rjyPmrqoqdWa",
        "cKUQcXqvqlpi",
        "5yEcw1WNqqba",
        "fFti-os3q50S",
        "rZluBHqTrBsr",
        "TI2obkIErEoL",
        "MrcuHtLfrNE7",
        "3yjoo8TXrel_",
        "_K1BI7TFrkI8",
        "y06mYhPSshni",
        "3EEG8rIbsmZ7",
        "NAWr3S4bsuQr",
        "t1D0mdFPsyjq",
        "p9fIgwMOs5Ej",
        "GJ50FYrIwh6F",
        "mlP4yZvywlIt",
        "R1E38f4Ewmns",
        "_BWJfnnPs7ST",
        "QpRLs8h-wz2Y",
        "-Gf5FsP0w3uz",
        "N5Tp4OSRw5qw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequisites**"
      ],
      "metadata": {
        "id": "nDQjLZNfk1FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Proficiency in Python**"
      ],
      "metadata": {
        "id": "3msGJ11omYli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Python is the primary programming language used in this project. All the implementation, including data preprocessing, API integration, and logic building, is done using Python.\n",
        "*   Knowing Python allows you to write clean, efficient code and use powerful libraries necessary for this project.\n",
        "\n"
      ],
      "metadata": {
        "id": "PTe_tbSsmc5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Familiarity with Jupyter Notebooks or Google Colab**"
      ],
      "metadata": {
        "id": "oG_U1kMUmgpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   These tools provide an interactive environment for coding, testing, and visualizing results.\n",
        "*   Since this project involves experimentation with various libraries and models, working in such environments will streamline development and debugging.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hi3u1meJmxnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Experience with Python Libraries**"
      ],
      "metadata": {
        "id": "llFTCdwcm1dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Pandas: Used for data manipulation and analysis. Helps in preparing datasets for training or processing queries.\n",
        "*   LangChain: A framework designed for building applications powered by language models, crucial for implementing the Retrieval Augmented Generation (RAG) workflow.\n",
        "*   Sklearn: Used for machine learning tasks such as clustering or similarity computation.\n",
        "*   Telegram Bot API: Facilitates building and deploying a chatbot on Telegram.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DbTlYoJam6Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Understanding and Using APIs**"
      ],
      "metadata": {
        "id": "EteoF5aFnF7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   OpenAI API: Provides access to GPT models, which power the generative responses.\n",
        "*   Gemini API: Another API that might be used for model integration or specific tasks in the project.\n",
        "*   Familiarity with APIs is essential to integrate external services into your chatbot for enhanced functionality.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dr0kBCGCnLsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Basic Knowledge of Machine Learning**"
      ],
      "metadata": {
        "id": "pEh0m5s2nVp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Machine Learning concepts, such as feature extraction and similarity computation, form the foundation for integrating retrieval mechanisms.\n",
        "*   Understanding ML ensures you can fine-tune or adapt models to improve chatbot performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "cX9E_T0rncZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Introduction to Generative AI and Retrieval-Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "lkQiVY3LnmRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Generative AI: The project centers around using AI to generate human-like responses. Knowing how these models work ensures you can leverage their capabilities effectively.\n",
        "*   RAG: Combines generative AI with a retrieval system to enhance responses by grounding them in factual data. Familiarity with this concept is crucial, as it forms the backbone of the chatbot.\n",
        "\n"
      ],
      "metadata": {
        "id": "8C6M86Y_nq7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Resources**"
      ],
      "metadata": {
        "id": "ueS3RtcxoN7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paid:**\n",
        "1.   https://www.udemy.com/course/complete-machine-learning-nlp-bootcamp-mlops-deployment/?couponCode=ST21MT121624\n",
        "2.   https://www.udemy.com/course/complete-generative-ai-course-with-langchain-and-huggingface/?couponCode=ST21MT121624\n",
        "\n",
        "**Free:**\n",
        "1. https://www.youtube.com/watch?v=v9bOWjwdTlg |  https://www.youtube.com/watch?v=Cri8__uGk-g\n",
        "2. https://www.youtube.com/watch?v=LZzq1zSL1bs&t=2s\n",
        "3. https://www.youtube.com/watch?v=fHFOANOHwh8\n",
        "4. https://www.youtube.com/watch?v=JxgmHe2NyeY&t=1s\n",
        "5. https://www.youtube.com/watch?v=d2kxUVwWWwU&t=1s\n",
        "6. https://www.youtube.com/watch?v=ENLEjGozrio&t=47s\n",
        "7. https://www.youtube.com/playlist?list=PLA1lVIthbM1D5I6r5uY2K89X1KD2w5LNh\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p5-jFb6coSJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Demo**"
      ],
      "metadata": {
        "id": "G0ucX816qTLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mount Drive**"
      ],
      "metadata": {
        "id": "dHGOLh8XqZfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpuG6oJKqX2e",
        "outputId": "e8f504f5-2958-4bc1-af4a-b7c86703531d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install Dependencies**"
      ],
      "metadata": {
        "id": "rjyPmrqoqdWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -qU langchain Faiss-gpu tiktoken sentence-transformers\n",
        "!pip install -qU trl Py7zr auto-gptq optimum\n",
        "!pip install -q rank_bm25\n",
        "!pip install -q PyPdf\n",
        "!pip install unstructured\n",
        "!pip install accelerate\n",
        "!pip install openai\n",
        "!pip install pinecone-client\n",
        "!pip install langchain_community\n",
        "!pip install --upgrade nltk\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install --upgrade --quiet  langchain-google-genai\n",
        "!pip install --upgrade --quiet langchain-text-splitters tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU0fHxI8qfZ3",
        "outputId": "10688581-de45-4ba3-8a5f-5e907c341ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured\n",
            "  Downloading unstructured-0.16.11-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.8.30)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pydantic<2.10.0,>=2.9.2 (from unstructured-client->unstructured)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured)\n",
            "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
            "Downloading unstructured-0.16.11-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=aae4f8fa903a2722a4f8e01ee2199ff74ea603545b9b7970b7c9ca054f35713f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pydantic-core, olefile, mypy-extensions, marshmallow, langdetect, jsonpath-python, emoji, backoff, aiofiles, typing-inspect, python-oxmsg, pydantic, unstructured-client, dataclasses-json, unstructured\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.1\n",
            "    Uninstalling pydantic_core-2.27.1:\n",
            "      Successfully uninstalled pydantic_core-2.27.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.3\n",
            "    Uninstalling pydantic-2.10.3:\n",
            "      Successfully uninstalled pydantic-2.10.3\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.14.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.23.1 mypy-extensions-1.0.0 olefile-0.47 pydantic-2.9.2 pydantic-core-2.23.4 python-iso639-2024.10.22 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.11.0 typing-inspect-0.9.0 unstructured-0.16.11 unstructured-client-0.28.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.5)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain_community\n",
            "Successfully installed httpx-sse-0.4.0 langchain_community-0.3.12 pydantic-settings-2.7.0 python-dotenv-1.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Libraries**"
      ],
      "metadata": {
        "id": "cKUQcXqvqlpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from langchain.embeddings import CacheBackedEmbeddings,HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
        "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.cache import InMemoryCache\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import prompt\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "from langchain import PromptTemplate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "jLtcHQUkqnCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skElEVOAqpLa",
        "outputId": "519fd341-2e4e-43e2-f71a-604a6659012f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Documents**"
      ],
      "metadata": {
        "id": "5yEcw1WNqqba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.directory import DirectoryLoader\n",
        "\n",
        "dir_loader = DirectoryLoader(\"/content/drive/MyDrive/Chatbot_Session\", glob=\"*.txt\")\n",
        "docs = dir_loader.load()\n",
        "print(f\"Number of documents are: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rogRt4C9qrlu",
        "outputId": "0aabd501-db37-4729-fbca-851bf0ed0a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents are: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chunking Documents**"
      ],
      "metadata": {
        "id": "fFti-os3q50S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Recursive Character Splitter"
      ],
      "metadata": {
        "id": "rZluBHqTrBsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300,\n",
        "                                      chunk_overlap=100,)\n",
        "\n",
        "esops_documents = text_splitter.transform_documents(docs)\n",
        "print(f\"Number of chunks in documents: {len(esops_documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_16sqZYq-ky",
        "outputId": "9f049c69-433a-4f09-e50f-fa66e4ed6369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks in documents: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Token Splitter"
      ],
      "metadata": {
        "id": "TI2obkIErEoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\",\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "esops_documents = text_splitter.transform_documents(docs)\n",
        "print(f\"number of chunks in documents : {len(esops_documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9omPV5czrL56",
        "outputId": "220aed01-e0ba-454b-b6df-520a9ffd372c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of chunks in documents : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Create Vector Store**"
      ],
      "metadata": {
        "id": "MrcuHtLfrNE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store = LocalFileStore(\"/content/drive/MyDrive/cache/\")\n",
        "embed_model_id = 'BAAI/bge-small-en-v1.5'\n",
        "core_embeddings_model = HuggingFaceEmbeddings(model_name=embed_model_id)\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(core_embeddings_model,\n",
        "                                                  store,\n",
        "                                                  namespace=embed_model_id)\n",
        "# Create VectorStore\n",
        "vectorstore = FAISS.from_documents(esops_documents,embedder)"
      ],
      "metadata": {
        "id": "o52-Rp5YrXJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Retriever**"
      ],
      "metadata": {
        "id": "3yjoo8TXrel_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve similar passage from vector store"
      ],
      "metadata": {
        "id": "_K1BI7TFrkI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"tell me about dr sanjeev kumar\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "print(len(embedding_vector))\n",
        "docs_resp = vectorstore.similarity_search_by_vector(embedding_vector,k=5)\n",
        "for page in docs_resp:\n",
        "  print(page.page_content)\n",
        "  print(\"\\n******************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx4hFW9WrlAr",
        "outputId": "ba229542-4b67-4672-d318-93e188fff67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "In scholarly activities, Dr. Sanjeev Kumar is a reviewer for SCI journals and co-editor of research books. Dr. Sanjeev Kumar coordinated and provided professional/research training on Machine Learning and Data Science courses. Dr. Sanjeev Kumar established a center of excellence in Super Computing and Deep Learning and assisted faculty members and scholars in writing research papers and patents. Additionally, Dr. Sanjeev Kumar has delivered expert talks on various topics in national/international workshops and has conducted faculty development programs funded by AICTE.\n",
            "\n",
            "Contact: sanjeev.kumar@ddn.upes.ac.in\n",
            "\n",
            "You can find scholarship details at https://www.upes.ac.in/admissions/scholarships.\n",
            "\n",
            "You can get fee structure at https://www.upes.ac.in/admissions/fee-structure.\n",
            "\n",
            "******************************\n",
            "\n",
            "Dr. Sanjeev Kumar has taught numerous undergraduate courses in Computer Science and Engineering, including Programming Fundamentals, Data Structures and Algorithms, Database Systems, Operating Systems, Computer Networks, Software Engineering, Artificial Intelligence, Soft computing (Neural Networks, Genetic Algorithms, Fuzzy Logic), Pattern Recognition, Microprocessor, Digital forensics, Digital Image Processing, Randomized Algorithms, and C Programming, among others. You can find the course curriculum at <page link>.\n",
            "\n",
            "Dr. Sanjeev Kumar has received several awards, including the \"C.V. Raman Award\" for Outstanding Academic Performance in Research, a Certificate of Appreciation from Texas Instruments for supporting students’ startup funding, the NPTEL Mentoring Award for the Course \"Introduction to Machine Learning,\" and recognition as a distinguished coach at the International Collegiate Programming Contest (ACM ICPC) across various universities. Dr. Sanjeev Kumar is also recognized as a Partner Faculty of Infosys Pvt. Ltd. with an honorable mention.\n",
            "\n",
            "******************************\n",
            "\n",
            "Page link: https://www.upes.ac.in/faculty/school-of-computer-science/dr-sanjeev-kumar contributed as an editor of research books and as a reviewer for SCI journals, with over 40 publications in reputable journals (including IEEE Transactions/SCI/ESCI/SCIE) and 7 patents that reflect success as a researcher.\n",
            "\n",
            "Dr. Sanjeev Kumar encourages student engagement through various teaching methodologies, including interactive lectures, hands-on projects, case studies, and discussions, thereby incorporating real-world examples and industry applications. This approach helps students connect theoretical concepts with practical applications, aims to develop problem-solving and critical thinking skills, and provides students opportunities to apply their knowledge through practical assignments and collaborative projects.\n",
            "\n",
            "Dr. Sanjeev Kumar has taught numerous undergraduate courses in Computer Science and Engineering, including Programming Fundamentals, Data Structures and Algorithms, Database Systems, Operating Systems, Computer Networks, Software Engineering, Artificial Intelligence, Soft computing (Neural Networks, Genetic Algorithms, Fuzzy Logic), Pattern Recognition, Microprocessor, Digital forensics, Digital Image Processing, Randomized Algorithms, and C Programming, among others. You can find the course curriculum at <page link>.\n",
            "\n",
            "******************************\n",
            "\n",
            "Dr. Aarushi Batra has over five years of work experience in academia and four years of research experience. Prior to joining UPES, Dr. Aarushi Batra taught at the School of Law, Bennett University, Greater Noida, and worked as an Assistant Professor at Shree Guru Gobind Singh Tricentenary (SGT) University and NorthCap University in Gurugram. While pursuing the doctorate, Dr. Aarushi Batra served as a teaching associate at Law Centre II at the Faculty of Law, University of Delhi.\n",
            "\n",
            "Dr. Aarushi Batra’s research interests include Sports Law, Constitutional Law, Gender Justice, Human Rights, Environmental Law, and Media Laws. Dr. Aarushi Batra's teaching philosophy is based on a continuous learning process, utilizing a combination of classroom note-taking, peer group discussions, and educational videos/documents. Dr. Aarushi Batra employs the Socratic method and promotes open discussions and Q&A sessions.\n",
            "\n",
            "Throughout her academic journey, Dr. Aarushi Batra has taught a wide array of subjects to both undergraduate and postgraduate students, including Constitutional Law, Women & Law, Sports Law, Media Law, Law of Contracts, Administrative Law, Right to Information, Law of Torts, Competition Law, Research Methodology, and International Environmental Law.\n",
            "\n",
            "Dr.\n",
            "\n",
            "******************************\n",
            "\n",
            "Page link: https://www.upes.ac.in/faculty/school\n",
            "\n",
            "of\n",
            "\n",
            "law/divya\n",
            "\n",
            "dwivedi\n",
            "\n",
            "Page link: https://www.upes.ac.in/faculty/school\n",
            "\n",
            "of\n",
            "\n",
            "law/divya\n",
            "\n",
            "dwivedi\n",
            "\n",
            "Divya Dwivedi is an Assistant Professor at the School of Law at UPES. Divya Dwivedi is an accomplished legal professional with a diverse academic background and a strong commitment to advancing knowledge in the field of law. Divya Dwivedi completed a BA LLB from Dr. Ram Manohar Lohiya National Law University (RMLNLU) and a post-graduation (LLM) from the prestigious National Law University, Jodhpur, specializing in International Trade Law. Divya Dwivedi is currently pursuing a Ph.D. from RMLNLU, focusing on the intersection of Data Privacy and Public Service Delivery in India. This doctoral journey is supported by a full-term Doctoral Fellowship from the Indian Council of Social Science Research (ICSSR), showcasing Divya Dwivedi's commitment to addressing contemporary challenges at the intersection of technology, governance, and privacy. Divya Dwivedi's profile reflects a dynamic individual who combines theoretical expertise with practical insights, positioning Divya Dwivedi as a valuable contributor to the legal and academic spheres.\n",
            "\n",
            "******************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Prompt Template**"
      ],
      "metadata": {
        "id": "y06mYhPSshni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = '''\n",
        "You are a virtual assistant created specifically for question answering.\n",
        "Your primary role is to answer questions given to you using only the context provided to you.\n",
        "\n",
        "Please adhere to the following guidelines:\n",
        "\n",
        "1. Respond accurately to user inquiries by referencing only the available context. Avoid referencing external information or creating answers based on general knowledge.\n",
        "2. Use only the CONTEXT given here to form answer for the QUESTION given after it.\n",
        "3. If the question is ambiguous then ask for more context.\n",
        "4. IF the answer is not present in context, reply with \"I don't have enough information to answer your question\"\n",
        "\n",
        "<context starts>\n",
        "Context: {context}\n",
        "<context ends>\n",
        "\n",
        "<question starts>\n",
        "Question: {question}\n",
        "<question ends>\n",
        "\n",
        "Do provide correct answer\n",
        "\n",
        "Helpful answer:\n",
        "'''\n",
        "\n",
        "input_variables = ['context', 'question']\n",
        "\n",
        "custom_prompt = PromptTemplate(template=PROMPT_TEMPLATE,\n",
        "                            input_variables=input_variables)"
      ],
      "metadata": {
        "id": "e_iPcrpSsjVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize LLM**"
      ],
      "metadata": {
        "id": "3EEG8rIbsmZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "GOOGLE_API_KEY=\"AIzaSyAg35lwWve1SBUiE2yLRj08H0G8BB0vizo\"\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "jSn8nrH3sqtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup Retrieval Chain**"
      ],
      "metadata": {
        "id": "NAWr3S4bsuQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handler = StdOutCallbackHandler()\n",
        "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":5}),\n",
        "    verbose=True,\n",
        "    callbacks=[handler],\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "43LIj-pisw0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing RAG on a query**"
      ],
      "metadata": {
        "id": "t1D0mdFPsyjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Without RAG**"
      ],
      "metadata": {
        "id": "p9fIgwMOs5Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "CV-yVmI2wpzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 1"
      ],
      "metadata": {
        "id": "GJ50FYrIwh6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"Tell me about Sanjeev Kumar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8VZUd4xes4i7",
        "outputId": "83324fcd-63b8-4f0c-98c7-0306a3289c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 73 ms, sys: 9.65 ms, total: 82.6 ms\n",
            "Wall time: 5.32 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "TmF5QMQFzKQ8",
        "outputId": "17d9e431-6838-4095-cf5d-849e28e2a51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sanjeev Kumar (1942-1985) was a highly acclaimed and influential Indian film actor, primarily known for his work in Hindi cinema.  Often considered one of the greatest actors of Indian cinema, he was renowned for his versatility and powerful portrayals of complex characters, often portraying ordinary, middle-class men with understated emotional depth.\\n\\nHere are some key aspects of his career and persona:\\n\\n* **Versatility:** He excelled in a wide range of roles, from comedic to tragic, and could portray both protagonists and antagonists with equal conviction.  He wasn't limited by a single type, making him a truly remarkable actor.\\n\\n* **Naturalistic Acting Style:** Unlike many of his contemporaries, Sanjeev Kumar's acting style was remarkably naturalistic. He avoided overt melodrama and instead conveyed emotions subtly through his expressions, body language, and understated delivery.  This gave his performances a remarkable realism and believability.\\n\\n* **Memorable Characters:** He played memorable characters in numerous critically acclaimed and commercially successful films.  He rarely played larger-than-life heroes; instead, he portrayed relatable characters dealing with everyday struggles and complexities.\\n\\n* **Notable Films:** Some of his most celebrated films include *Angoor* (a hilarious adaptation of Shakespeare's *The Comedy of Errors*), *Koshish* (a poignant portrayal of a deaf and mute couple), *Aakhir Kyon?* (a complex drama exploring societal norms), *Namak Haram* (a compelling tale of friendship and betrayal), and *Pati Patni Aur Woh* (a comedic take on infidelity).\\n\\n* **Critical Acclaim & Awards:** He received widespread critical acclaim throughout his career and won numerous awards, though not as many as his talent arguably deserved. His performances were consistently praised for their depth and authenticity.\\n\\n* **Tragic End:** His relatively early death in 1985 due to a heart attack left a void in Indian cinema. His legacy continues to inspire actors and filmmakers, and his films remain highly regarded and cherished by audiences.\\n\\n\\nIn essence, Sanjeev Kumar was more than just a star; he was a highly respected and influential figure who redefined the landscape of Hindi cinema acting.  He remains an icon, celebrated for his exceptional talent, dedication to his craft, and his ability to connect with audiences through deeply human and realistic portrayals.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 2"
      ],
      "metadata": {
        "id": "mlP4yZvywlIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"Explain how AI works\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3-JoL0Mawq-j",
        "outputId": "b8f0e0c2-623b-4f32-8709-bb8b5e4cd2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 83.6 ms, sys: 9.88 ms, total: 93.5 ms\n",
            "Wall time: 5.69 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "KXgVNBiyzPfY",
        "outputId": "c97593e7-d14b-4aea-d16a-f02f8bad11e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI works by mimicking human intelligence through various techniques, primarily focusing on learning, reasoning, and self-correction.  There\\'s no single \"how\" because AI encompasses many different approaches. Here\\'s a breakdown of some key concepts:\\n\\n**1. Data is King:** AI systems learn from data.  The more relevant and high-quality data you feed an AI, the better it performs.  This data can be anything from images and text to sensor readings and financial transactions.\\n\\n**2. Algorithms are the Tools:**  Algorithms are sets of rules and statistical techniques that AI uses to process data and learn patterns.  Different algorithms are suited for different tasks.  Some common types include:\\n\\n* **Machine Learning (ML):**  This is the most common approach.  ML algorithms allow the system to learn from data without explicit programming.  They identify patterns, make predictions, and improve their accuracy over time.  Key sub-types include:\\n    * **Supervised Learning:** The algorithm is trained on labeled data (data with known inputs and outputs).  Examples include image classification (identifying cats vs. dogs) and spam detection.\\n    * **Unsupervised Learning:** The algorithm is trained on unlabeled data and aims to discover hidden patterns or structures.  Examples include clustering similar customers based on purchase history.\\n    * **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment and receiving rewards or penalties for its actions.  Examples include game playing (AlphaGo) and robotics.\\n\\n* **Deep Learning (DL):** A subset of ML that uses artificial neural networks with multiple layers (hence \"deep\"). These networks can learn complex patterns from vast amounts of data, excelling in areas like image recognition, natural language processing, and speech recognition.\\n\\n* **Expert Systems:**  These systems mimic the decision-making of a human expert in a specific domain using a set of rules and knowledge bases.  They are less common now but were prevalent earlier in AI\\'s history.\\n\\n**3. Models are the Result:**  Through the application of algorithms to data, AI systems create \"models.\" These models are mathematical representations of the patterns learned from the data.  They are used to make predictions or decisions on new, unseen data.\\n\\n**4. Evaluation and Improvement:**  AI systems are constantly evaluated on their performance.  Metrics are used to measure accuracy, precision, and other relevant factors.  Based on this evaluation, the algorithms and models are refined and improved – a process often called \"training\" or \"fine-tuning.\"\\n\\n\\n**In simple terms:** Imagine teaching a dog a trick.  You show the dog (the AI) examples (the data) of the trick, reward it when it gets it right (reinforcement learning), and correct it when it\\'s wrong.  Eventually, the dog learns the trick (creates a model) and can perform it reliably.  AI works similarly, but with algorithms and data instead of treats and corrections.\\n\\nIt\\'s important to note that AI is still a developing field.  While current AI systems can achieve impressive results in specific tasks, they are far from possessing true human-level intelligence or consciousness.  They are tools that can automate tasks, improve decision-making, and uncover insights from data, but their capabilities are bounded by the data they are trained on and the algorithms they employ.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 3"
      ],
      "metadata": {
        "id": "R1E38f4Ewmns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"Do you know about the Runway initiative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2wV409jpwt7v",
        "outputId": "5e6d5adb-a461-4bdd-b714-34f15c6be6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 57.9 ms, sys: 7.6 ms, total: 65.5 ms\n",
            "Wall time: 3.59 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "kDyMTSlmzjFc",
        "outputId": "67d8bf6a-5d7e-4767-ccdb-27eeaaaefc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, I do.  \"Runway\" can refer to a few different things, so to best answer your question, I need some clarification.  Are you referring to:\\n\\n* **Runway ML:** This is a popular AI platform offering tools and resources for creating and deploying machine learning models, particularly focused on video and image generation.  They offer various models and services accessible through their web interface or API.\\n\\n* **Runway (a company in a different context):**  There might be other companies or initiatives using the name \"Runway\" in a different field.  Could you be thinking of something else?\\n\\nPlease specify which \"Runway\" you\\'re interested in, and I\\'ll be happy to provide more information.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. With RAG**"
      ],
      "metadata": {
        "id": "_BWJfnnPs7ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 1"
      ],
      "metadata": {
        "id": "QpRLs8h-wz2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"\"\"Tell me about Sanjeev Kumar\"\"\"\n",
        "response = qa_with_sources_chain({\"query\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZAkK29Ps85_",
        "outputId": "bd2eb18c-94da-44cf-b966-f83e9d9642ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "CPU times: user 183 ms, sys: 21.7 ms, total: 205 ms\n",
            "Wall time: 1.59 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "_QwOj2wWs_qS",
        "outputId": "ba5cbda6-1b33-4102-de53-4c886d725ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dr. Sanjeev Kumar is a professor who has taught numerous undergraduate Computer Science and Engineering courses, including Programming Fundamentals, Data Structures and Algorithms, Database Systems, and many others.  He has received several awards, including the \"C.V. Raman Award\" and the NPTEL Mentoring Award. He\\'s a reviewer for SCI journals, co-editor of research books, and a Partner Faculty of Infosys Pvt. Ltd.  He\\'s also coordinated professional/research training on Machine Learning and Data Science, established a center of excellence in Super Computing and Deep Learning, and given expert talks at national/international workshops.  He has over 40 publications in reputable journals and 7 patents.  His teaching methods include interactive lectures, hands-on projects, and case studies.  His contact email is sanjeev.kumar@ddn.upes.ac.in.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 2"
      ],
      "metadata": {
        "id": "-Gf5FsP0w3uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"\"\"Explain how AI workss\"\"\"\n",
        "response = qa_with_sources_chain({\"query\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI7pF9Lsw46t",
        "outputId": "95d74b21-9611-40cd-fed2-13bc37d73859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "CPU times: user 65.6 ms, sys: 663 µs, total: 66.3 ms\n",
            "Wall time: 565 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OZmAkg35w5XG",
        "outputId": "6da424d7-551b-4f82-9b9c-77c8c3101af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't have enough information to answer your question.  The provided text mentions that Dr. Sanjeev Kumar teaches Artificial Intelligence as a course, but it does not explain how AI works.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 3"
      ],
      "metadata": {
        "id": "N5Tp4OSRw5qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"\"\"Do you know about the Runway initiative\"\"\"\n",
        "response = qa_with_sources_chain({\"query\":query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0Yr2yx_w6u2",
        "outputId": "e3f4929a-de4c-4710-b001-e4065221becd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "CPU times: user 46.9 ms, sys: 1.83 ms, total: 48.7 ms\n",
            "Wall time: 641 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3rQbD0USw7qd",
        "outputId": "c54dc921-19e1-418e-fe39-1d804f8595db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, the Runway initiative at UPES is a 10-week incubation program that supports idea-stage and early-stage startups.  It helps refine business ideas, prepare startups for market entry, and connect them with potential investors.  The program provides mentorship and access to labs and workspace.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}